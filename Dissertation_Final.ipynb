{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Parts of this has been pipeline has been implemented alongside https://github.com/zaidalyafeai/zaidalyafeai.github.io/blob/3523bffa1f876fd6d67fe599dfbe52570290df6d/sketcher/Sketcher.ipynb"
      ],
      "metadata": {
        "id": "s6G10GSzW_cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beginning: Library and Module imports"
      ],
      "metadata": {
        "id": "_ZunUYLa09A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessary Libraries.\n",
        "There will be deviations within this notebook depending on the model trained and the imported dataset methods.\n"
      ],
      "metadata": {
        "id": "EspWC5p74wGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import numpy\n",
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZEqUbnEf40Ip"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google Drive. Feel free to edit the directory depending on your Google Drive structure, or directly upload the dataset to the runtime."
      ],
      "metadata": {
        "id": "2DskSPfGJqm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvFhRmVMGbq2",
        "outputId": "5e08bed0-ac5b-4ac7-f503-1900d7e81b0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Dissertation/Dataset_for_training'\n",
        "test_path = '/content/drive/MyDrive/Dissertation/Dataset_for_testing/'"
      ],
      "metadata": {
        "id": "nEu9FJzKLbBq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can grab the individual .npy files for training and testing the model from the Google Cloud. https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap \n",
        "\n",
        "You'll need to put these inside of your drive. You can get an absolute path by using the file system window on the left-hand side of the page and right-clicking and copying the path. Change the relevant training and testing path. \n",
        "\n",
        "For locating the sketches within these files for later examination of specific sketches within the .npy files, you can grab batches of the ndjson files from kaggle (https://www.kaggle.com/datasets/google/tinyquickdraw) or from (https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/simplified), these are simplified and preprocessed."
      ],
      "metadata": {
        "id": "IPRUnnmvLf_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ndjson\n",
        "import ndjson\n",
        "json_path = \"/content/drive/MyDrive/Dissertation/ndjson/sword.ndjson\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nidAme_d5EO1",
        "outputId": "1fa0bcbf-50b0-44f5-f66c-efbd4d1602b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ndjson\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: ndjson\n",
            "Successfully installed ndjson-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_sketch_strokes = 2"
      ],
      "metadata": {
        "id": "yFhl8hi54mkB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script reads the json objects from within the ndjson files within the relevant path. Below is the explanation of variables.\n",
        "\n",
        "\n",
        "*   json_path - location of ndjson file\n",
        "*   desired_sketch_strokes - the number of strokes that make up a sketch\n",
        "*   line_num - starts at 1, increments each loop. This is appended to the end of json_object, indicates which line it resides upon in the ndjson file\n",
        "*   json_data - all the lines within the ndjson file\n",
        "*   draw_array - the drawing representation (an array) pulled from the current json_object\n",
        "*   json_object - the current line iterated on within the ndjson file\n",
        "*   list_sketches - a list of numbers that reference the line number within the ndjson file that match the desired number of strokes within a sketch\n"
      ],
      "metadata": {
        "id": "bVSJwECKE-4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sketch_line_nums(stroke_num):\n",
        "  list_sketches = []\n",
        "  line_num = 1\n",
        "  with open (json_path) as jsonfile:\n",
        "    json_data = ndjson.load(jsonfile)\n",
        "    for json_object in json_data:\n",
        "      draw_array = (json_object[\"drawing\"])\n",
        "      json_object['line'] = line_num\n",
        "      line_num += 1\n",
        "      if len(draw_array) == stroke_num:\n",
        "        list_sketches.append(json_object['line'])\n",
        "    return list_sketches"
      ],
      "metadata": {
        "id": "X5Lka_WIKPK2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_sketches = extract_sketch_line_nums(desired_sketch_strokes)\n",
        "print(len(list_sketches))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhM4eCDK_k2",
        "outputId": "fefe43ed-470b-480a-a561-b58833d4623f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Functions\n"
      ],
      "metadata": {
        "id": "eLkezU0GTlNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_files(max, files):\n",
        "\n",
        "    #set x and y for test, shaped for the npy files (QuickDraw npy files are shaped x[image],[resolution (28, 28)] y is labels (based on class names))\n",
        "    x = numpy.empty([0, 784])\n",
        "    y = numpy.empty([0])\n",
        "    classes = []\n",
        "\n",
        "    #load each file into x and y\n",
        "    for sketchx, file in enumerate(files):\n",
        "        data = numpy.load(file)\n",
        "        data = data[0: max, :]\n",
        "        labels = numpy.full(data.shape[0], sketchx)\n",
        "        x = numpy.concatenate((x, data), axis=0)\n",
        "        y = numpy.append(y, labels)\n",
        "        file_name, ext = os.path.splitext(os.path.basename(file))\n",
        "        classes.append(file_name)  # add filenames to the back of list, remove the extension (.npy)\n",
        "       \n",
        "    #clear data and labels at the end of the loop\n",
        "    data = None\n",
        "    labels = None\n",
        "\n",
        "    return y, x, classes"
      ],
      "metadata": {
        "id": "v5yNB-DysGJ2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should ONLY be called for the dataset used for training, testing and validation of the Convolutional Neural Network."
      ],
      "metadata": {
        "id": "39vpXsVnz3a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_init_network(data_path): \n",
        "    all_files = glob.glob(os.path.join(data_path, '*.npy'))\n",
        "    validation_from_files=0.2\n",
        "    max_sketch_per_class= 4000\n",
        "\n",
        "    y, x, classes = load_files(max_sketch_per_class, all_files)\n",
        "    \n",
        "    #randomize the dataset using the permutations\n",
        "    randomisation = numpy.random.permutation(y.shape[0])\n",
        "    x = x[randomisation, :]\n",
        "    y = y[randomisation]\n",
        "\n",
        "    # we're using around 20% for spliting into test/training\n",
        "    split_size = int(x.shape[0]/100*(validation_from_files*100))\n",
        "\n",
        "    x_test = x[0:split_size, :]\n",
        "    y_test = y[0:split_size]\n",
        "\n",
        "    x_train = x[split_size:x.shape[0], :]\n",
        "    y_train = y[split_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, classes"
      ],
      "metadata": {
        "id": "xV3WQcw_JVE8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may fail due to System RAM, try running it once or twice."
      ],
      "metadata": {
        "id": "wCErfW9kZyNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test, classes = data_init_network(train_path)"
      ],
      "metadata": {
        "id": "I-uXs3ViVhKj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, load the data within the stroke-testing folder which will be used to specifically indentify the strokes, non-randomised with no classes extracted. You only need testing, there's no need for splitting this."
      ],
      "metadata": {
        "id": "sYql9IheKhya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_init_strokes(path):\n",
        "    stroke_files = glob.glob(os.path.join(path, '*.npy'))\n",
        "    max_sketch_per_class= 8000\n",
        "    y, x, classes = load_files(max_sketch_per_class, stroke_files)\n",
        "    x_test = x[0:x.shape[0], :]\n",
        "    y_test = y[0:x.shape[0]]\n",
        "\n",
        "    return x_test, y_test"
      ],
      "metadata": {
        "id": "M4smFRwTKhPc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_test, r_test = data_init_strokes(test_path)"
      ],
      "metadata": {
        "id": "O-8erUoiKjnR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes)\n",
        "print(x_train.shape)\n",
        "print(e_test.shape) # should be num of test files x 8000\n",
        "print(r_test.shape)\n",
        "print(\"First part of the array is number of images, second number is the number of neurons required within input laye (after re-shaping)\")"
      ],
      "metadata": {
        "id": "br5t5YWxWFBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93fd365-c7e2-441f-e3d2-4a5be44d3b7e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['full_numpy_bitmap_sweater', 'full_numpy_bitmap_swing set', 'full_numpy_bitmap_sword', 'full_numpy_bitmap_syringe', 'full_numpy_bitmap_t-shirt', 'full_numpy_bitmap_table', 'full_numpy_bitmap_teapot', 'full_numpy_bitmap_telephone', 'full_numpy_bitmap_television', 'full_numpy_bitmap_teddy-bear', 'full_numpy_bitmap_tent', 'full_numpy_bitmap_tiger', 'full_numpy_bitmap_tennis racquet', 'full_numpy_bitmap_toilet', 'full_numpy_bitmap_toaster', 'full_numpy_bitmap_tooth', 'full_numpy_bitmap_toe', 'full_numpy_bitmap_toothbrush', 'full_numpy_bitmap_toothpaste', 'full_numpy_bitmap_tractor', 'full_numpy_bitmap_train', 'full_numpy_bitmap_tornado', 'full_numpy_bitmap_traffic light', 'full_numpy_bitmap_triangle', 'full_numpy_bitmap_trombone', 'full_numpy_bitmap_tree', 'full_numpy_bitmap_truck', 'full_numpy_bitmap_umbrella', 'full_numpy_bitmap_underwear', 'full_numpy_bitmap_trumpet', 'full_numpy_bitmap_wine bottle', 'full_numpy_bitmap_windmill', 'full_numpy_bitmap_whale', 'full_numpy_bitmap_wheel', 'full_numpy_bitmap_zigzag', 'full_numpy_bitmap_zebra', 'full_numpy_bitmap_wine glass', 'full_numpy_bitmap_wristwatch', 'full_numpy_bitmap_yoga', 'full_numpy_bitmap_pear', 'full_numpy_bitmap_pencil', 'full_numpy_bitmap_peas', 'full_numpy_bitmap_pillow', 'full_numpy_bitmap_piano', 'full_numpy_bitmap_pickup truck', 'full_numpy_bitmap_pineapple', 'full_numpy_bitmap_pizza', 'full_numpy_bitmap_pool', 'full_numpy_bitmap_pig', 'full_numpy_bitmap_picture frame', 'full_numpy_bitmap_penguin', 'full_numpy_bitmap_postcard', 'full_numpy_bitmap_pond', 'full_numpy_bitmap_pliers', 'full_numpy_bitmap_popsicle', 'full_numpy_bitmap_rabbit', 'full_numpy_bitmap_raccoon', 'full_numpy_bitmap_police car', 'full_numpy_bitmap_purse', 'full_numpy_bitmap_power outlet', 'full_numpy_bitmap_potato', 'full_numpy_bitmap_bathtub', 'full_numpy_bitmap_basket', 'full_numpy_bitmap_basketball', 'full_numpy_bitmap_bee', 'full_numpy_bitmap_bed', 'full_numpy_bitmap_beard', 'full_numpy_bitmap_bench', 'full_numpy_bitmap_blackberry', 'full_numpy_bitmap_boomerang', 'full_numpy_bitmap_book', 'full_numpy_bitmap_bracelet', 'full_numpy_bitmap_birthday cake', 'full_numpy_bitmap_brain', 'full_numpy_bitmap_bridge', 'full_numpy_bitmap_camera', 'full_numpy_bitmap_camouflage', 'full_numpy_bitmap_camel', 'full_numpy_bitmap_calendar', 'full_numpy_bitmap_cello', 'full_numpy_bitmap_chandelier', 'full_numpy_bitmap_circle', 'full_numpy_bitmap_church', 'full_numpy_bitmap_clarinet', 'full_numpy_bitmap_cloud', 'full_numpy_bitmap_clock', 'full_numpy_bitmap_computer', 'full_numpy_bitmap_compass', 'full_numpy_bitmap_coffee cup', 'full_numpy_bitmap_fence', 'full_numpy_bitmap_finger', 'full_numpy_bitmap_fire hydrant', 'full_numpy_bitmap_fireplace', 'full_numpy_bitmap_fish', 'full_numpy_bitmap_firetruck', 'full_numpy_bitmap_flamingo', 'full_numpy_bitmap_flashlight', 'full_numpy_bitmap_flip flops', 'full_numpy_bitmap_floor lamp', 'full_numpy_bitmap_flower']\n",
            "(320000, 784)\n",
            "(64000, 784)\n",
            "(64000,)\n",
            "First part of the array is number of images, second number is the number of neurons required within input laye (after re-shaping)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-created sketches"
      ],
      "metadata": {
        "id": "lucveBFG0_Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SIDE NOTE SECTION:** \n",
        "Not neccessary. If you need to convert a drawing to the proper format that is also then more suitable for matching up with the training data, then the code below will let you. This will not work with this solution."
      ],
      "metadata": {
        "id": "s5GtwgLUa-Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can save this image for later use if desired."
      ],
      "metadata": {
        "id": "xPxPGSA0xkGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "filename = \"/content/drive/MyDrive/Dissertation/User-created/3strokeflower.png\"\n",
        "sketch = Image.open(filename)\n",
        "sketch.thumbnail((28,28), Image.ANTIALIAS)\n",
        "resized = Image.new(\"1\", (28,28), \"white\")\n",
        "resized.paste(sketch, (int(((28,28)[0] - sketch.size[0]) / 2), int(((28,28)[1] - sketch.size[1]) / 2)))\n",
        "resized.save(file_name, \"PNG\")\n",
        "resized.show() # only use this sparingly and if you need to check individual images"
      ],
      "metadata": {
        "id": "7SD834sobhH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "e2b86a46-9c87-408b-8e26-916d96653537"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=1 size=28x28 at 0x7FD56981D210>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcAQAAAABaduI5AAAAc0lEQVR4nD3NMQ6CQBQG4Xm/SGVDQmKlWHkoa49CuJoNhQegsLSEsEri7vpsiM0kXzXmMAlgTQTxviLfnxFDjSXXrNSCzbuQFT6g57JFVdmhg30RMWM5FkHcBLnpR/nphTaXI/i9GUV4YO42Cf/fEohy5Q9PxShH168aOAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code converts an image into an array, directly ready for use at prediction."
      ],
      "metadata": {
        "id": "gltgkfy83WxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/Dissertation/User-created 3strokeflower.png\"\n",
        "img = tf.keras.utils.load_img(filename, color_mode=\"grayscale\", target_size=(28,28))\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "img.show()"
      ],
      "metadata": {
        "id": "xjHZ6RBz2NmF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "outputId": "4fcbb1b1-c529-4218-98b4-11ba0ed87276"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7FD56981F550>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAkUlEQVR4nJWSSxIEIQhDE6rvf+XMQgPCzFjVroTHx4AU/p+4sDeQYDPVGACBgBpcGVo3cdEsK6lCdsYzC+LoE2ZEB5nJ4bUdjbEpQTRWzWkp2Y7KSGqXlfLxaLJ/j28OwUJVig/I3eCoHsUsg360x1ebOI1aGd2yXE+GizmEbwgwF2nHWESz48Le/b5LnWvmOB8ZcDQ7lTgQVAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-shaping, pre-processing and normalisation"
      ],
      "metadata": {
        "id": "KGOdkIdo12tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shapes the npy files within train and test to (shape) 28, 28, 1\n",
        "# these are representations of 1 bit deep images at 28,28 sizes (black and white)\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "e_test = e_test.reshape(e_test.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "e_test /= 255.0\n",
        "# Here, we need to convert a class vector to binary class matrices\n",
        "# Num of Files is referred to as classes, it will output the file associated as a class when predicting\n",
        "numofFiles= len(classes)\n",
        "y_train = keras.utils.to_categorical(y_train, numofFiles)\n",
        "y_test = keras.utils.to_categorical(y_test, numofFiles)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(e_test.shape)"
      ],
      "metadata": {
        "id": "7u1AhyHM0Qxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be5b348-15c5-4bac-a34e-9b1a4b3c6b44"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(320000, 28, 28, 1)\n",
            "(80000, 28, 28, 1)\n",
            "(64000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "ZAXZfhTh2C6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining models. There will be two models tested. One from a basic Sequential Convolutional Neural Network. Last layer must be softmax, this is not a binary classification problem."
      ],
      "metadata": {
        "id": "1xSv5VC0fw4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "conrad = keras.Sequential()\n",
        "conrad.add(layers.Convolution2D(16, (3, 3),  padding='same', input_shape=x_train.shape[1:], activation='relu'))\n",
        "conrad.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "#conrad.add(layers.Convolution2D(16, (3, 3),  padding='same', input_shape=x_train.shape[1:], activation='relu')) # [1:] everything after the first index of x.shape\n",
        "conrad.add(layers.Convolution2D(32, (3, 3),  padding='same', activation='relu'))\n",
        "conrad.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "conrad.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "conrad.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "conrad.add(layers.Convolution2D(128, (3, 3), padding='same', activation= 'relu'))\n",
        "conrad.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
        "conrad.add(layers.Flatten())\n",
        "conrad.add(layers.Dense(128, activation='relu'))  \n",
        "conrad.add(layers.Dense(numofFiles, activation='softmax'))  # non-binary classification\n",
        "# Train model\n",
        "adam = tf.optimizers.Adam()\n",
        "conrad.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "print(conrad.summary())"
      ],
      "metadata": {
        "id": "io_2ofJ3fv0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc2aed2-f737-412d-ae7b-39aaa31bab5b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 3, 3, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 1, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 126,564\n",
            "Trainable params: 126,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conrad.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 128, verbose=2, epochs=10)"
      ],
      "metadata": {
        "id": "Mxsmx3Nsgb7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b911150d-d80c-45f7-b553-b8b76058298e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2250/2250 - 21s - loss: 2.0054 - accuracy: 0.4993 - val_loss: 1.4964 - val_accuracy: 0.6173 - 21s/epoch - 9ms/step\n",
            "Epoch 2/10\n",
            "2250/2250 - 9s - loss: 1.3495 - accuracy: 0.6512 - val_loss: 1.2663 - val_accuracy: 0.6705 - 9s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "2250/2250 - 9s - loss: 1.1922 - accuracy: 0.6893 - val_loss: 1.1971 - val_accuracy: 0.6903 - 9s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "2250/2250 - 10s - loss: 1.1025 - accuracy: 0.7103 - val_loss: 1.1343 - val_accuracy: 0.7052 - 10s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "2250/2250 - 9s - loss: 1.0421 - accuracy: 0.7260 - val_loss: 1.0875 - val_accuracy: 0.7173 - 9s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "2250/2250 - 10s - loss: 0.9968 - accuracy: 0.7361 - val_loss: 1.0730 - val_accuracy: 0.7213 - 10s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "2250/2250 - 10s - loss: 0.9578 - accuracy: 0.7450 - val_loss: 1.0694 - val_accuracy: 0.7187 - 10s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "2250/2250 - 10s - loss: 0.9291 - accuracy: 0.7516 - val_loss: 1.0427 - val_accuracy: 0.7282 - 10s/epoch - 5ms/step\n",
            "Epoch 9/10\n",
            "2250/2250 - 9s - loss: 0.9037 - accuracy: 0.7588 - val_loss: 1.0595 - val_accuracy: 0.7262 - 9s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "2250/2250 - 9s - loss: 0.8827 - accuracy: 0.7634 - val_loss: 1.0277 - val_accuracy: 0.7357 - 9s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5609f97b0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = conrad.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))\n",
        "print('Loss value: {:0.2f}'.format(score[0]))"
      ],
      "metadata": {
        "id": "GRIwqJU2l6Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e0eacb-e109-4e5c-86c5-b9d8da9fb507"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuarcy: 73.16%\n",
            "Loss value: 1.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del score # clears up some memory"
      ],
      "metadata": {
        "id": "5L9bAWBZpYpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Sketch prediction"
      ],
      "metadata": {
        "id": "XRStVSWN3kvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For using the sketch-testing dataset, the below code will search for the index of the array. This is for individual testing of sketches. Unless you use a value above 4000, this has the potential to be the same data used within the training and testing of the model. Upon passing 8000, it should no longer be used."
      ],
      "metadata": {
        "id": "YH2IgpBp433_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_sketches[0])\n",
        "# This is the line within the NDJSON file. \n",
        "# As each object has a total of 8000, it should be noted that when passing 8000 lines, you have moved to a new class\n",
        "# For example, \"Clock\" sketches  may exist between 1-8000, then begins a new object, etc."
      ],
      "metadata": {
        "id": "0hnnvhN18qjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7bd0ae-2f50-4d39-a073-2f3cd50b381b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "uid = list_sketches[0]\n",
        "print(uid)\n",
        "sketch = e_test[uid]\n",
        "plt.imshow(sketch.squeeze()) \n",
        "pred = conrad.predict(numpy.expand_dims(sketch, axis=0))[0]\n",
        "indices_array = (-pred).argsort()[:5]\n",
        "sketch_prediction = [classes[x] for x in indices_array]\n",
        "print(sketch_prediction)\n",
        "#round to 2 decimal places\n",
        "print(\"%.2f\" % round(pred[indices_array[0]], 2))\n",
        "print(\"%.2f\" % round(pred[indices_array[1]], 2))\n",
        "print(\"%.2f\" % round(pred[indices_array[2]], 2))\n",
        "print(\"%.2f\" % round(pred[indices_array[3]], 2))\n",
        "print(\"%.2f\" % round(pred[indices_array[4]], 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "pLku9jpxAgbp",
        "outputId": "a5fc62d9-d219-414e-e47e-eefcb854e3bd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['full_numpy_bitmap_sword', 'full_numpy_bitmap_chandelier', 'full_numpy_bitmap_syringe', 'full_numpy_bitmap_flashlight', 'full_numpy_bitmap_floor lamp']\n",
            "0.93\n",
            "0.02\n",
            "0.01\n",
            "0.01\n",
            "0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conrad.save('/content/drive/MyDrive/Dissertation/CONRAD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN7XGKYCkvlZ",
        "outputId": "707379b5-12a0-400a-b835-4360a687df50"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}